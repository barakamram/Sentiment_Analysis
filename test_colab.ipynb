{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from emotion_recognition import EmotionRecognizer\n", "from deep_emotion_recognition import DeepEmotionRecognizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pyaudio\n", "import os\n", "import wave\n", "from sys import byteorder\n", "from array import array\n", "from struct import pack\n", "from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from utils import get_best_estimators\n", "import warnings\n", "# import tensorflow as tf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["warnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["THRESHOLD = 500\n", "CHUNK_SIZE = 1024\n", "FORMAT = pyaudio.paInt16\n", "RATE = 16000"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["SILENCE = 30"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def is_silent(snd_data):\n", "    \"Returns 'True' if below the 'silent' threshold\"\n", "    return max(snd_data) < THRESHOLD"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def normalize(snd_data):\n", "    \"Average the volume out\"\n", "    MAXIMUM = 16384\n", "    times = float(MAXIMUM) / max(abs(i) for i in snd_data)\n", "    r = array('h')\n", "    for i in snd_data:\n", "        r.append(int(i * times))\n", "    return r"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def trim(snd_data):\n", "    \"Trim the blank spots at the start and end\"\n", "    def _trim(snd_data):\n", "        snd_started = False\n", "        r = array('h')\n", "        for i in snd_data:\n", "            if not snd_started and abs(i) > THRESHOLD:\n", "                snd_started = True\n", "                r.append(i)\n", "            elif snd_started:\n", "                r.append(i)\n", "        return r\n\n", "    # Trim to the left\n", "    snd_data = _trim(snd_data)\n\n", "    # Trim to the right\n", "    snd_data.reverse()\n", "    snd_data = _trim(snd_data)\n", "    snd_data.reverse()\n", "    return snd_data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def add_silence(snd_data, seconds):\n", "    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n", "    r = array('h', [0 for i in range(int(seconds * RATE))])\n", "    r.extend(snd_data)\n", "    r.extend([0 for i in range(int(seconds * RATE))])\n", "    return r"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def record():\n", "    \"\"\"\n", "    Record a word or words from the microphone and \n", "    return the data as an array of signed shorts.\n", "    Normalizes the audio, trims silence from the \n", "    start and end, and pads with 0.5 seconds of \n", "    blank sound to make sure VLC et al can play \n", "    it without getting chopped off.\n", "    \"\"\"\n", "    p = pyaudio.PyAudio()\n", "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n", "                    input=True, output=True,\n", "                    frames_per_buffer=CHUNK_SIZE)\n", "    num_silent = 0\n", "    snd_started = False\n", "    r = array('h')\n", "    while 1:\n", "        # little endian, signed short\n", "        snd_data = array('h', stream.read(CHUNK_SIZE))\n", "        if byteorder == 'big':\n", "            snd_data.byteswap()\n", "        r.extend(snd_data)\n", "        silent = is_silent(snd_data)\n", "        if silent and snd_started:\n", "            num_silent += 1\n", "        elif not silent and not snd_started:\n", "            snd_started = True\n", "        if snd_started and num_silent > SILENCE:\n", "            break\n", "    sample_width = p.get_sample_size(FORMAT)\n", "    stream.stop_stream()\n", "    stream.close()\n", "    p.terminate()\n", "    r = normalize(r)\n", "    r = trim(r)\n", "    r = add_silence(r, 0.5)\n", "    return sample_width, r"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def record_to_file(path):\n", "    \"Records from the microphone and outputs the resulting data to 'path'\"\n", "    sample_width, data = record()\n", "    data = pack('<' + ('h' * len(data)), *data)\n", "    wf = wave.open(path, 'wb')\n", "    wf.setnchannels(1)\n", "    wf.setsampwidth(sample_width)\n", "    wf.setframerate(RATE)\n", "    wf.writeframes(data)\n", "    wf.close()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_estimators_name(estimators):\n", "    result = ['\"{}\"'.format(estimator.__class__.__name__) for estimator, _, _ in estimators]\n", "    return ','.join(result), {estimator_name.strip('\"'): estimator for estimator_name, (estimator, _, _) in\n", "                              zip(result, estimators)}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    estimators = get_best_estimators(True)\n", "    estimators_str, estimator_dict = get_estimators_name(estimators)\n", "    import argparse\n", "    parser = argparse.ArgumentParser(description=\"\"\"\n", "                                    Testing emotion recognition system using your voice, \n", "                                    please consider changing the model and/or parameters as you wish.\n", "                                    \"\"\")\n", "    parser.add_argument(\"-e\", \"--emotions\", help=\n", "    \"\"\"Emotions to recognize separated by a comma ',', available emotions are\n", "                                            \"neutral\", \"calm\", \"happy\" \"sad\", \"angry\", \"fear\", \"disgust\", \"ps\" (pleasant surprise)\n", "                                            and \"boredom\", default is \"sad,neutral,happy\"\n", "                                            \"\"\", default=\"sad,neutral,happy,angry\")\n", "    parser.add_argument(\"-m\", \"--model\", help=\n", "    \"\"\"\n", "                                        The model to use, 8 models available are: {},\n", "                                        default is \"BaggingClassifier\"\n", "                                        \"\"\".format(estimators_str), default=\"BaggingClassifier\")\n\n", "    # Parse the arguments passed\n", "    args = parser.parse_args()\n", "    lst = ['sad','neutral','happy']\n", "    # 'excited', 'assertive', 'encouraging', 'anxious', 'apologetic', 'concerned'\n", "    features = [\"mfcc\", \"chroma\", \"mel\"]\n", "    # detector = EmotionRecognizer(estimator_dict[args.model], emotions=lst, features=features, verbose=0)\n", "    # detector.train()\n", "    # print(\"Test accuracy score: {:.3f}%\".format(detector.test_score() * 100))\n", "    # print(\"Please talk\")\n", "    # deeprec = DeepEmotionRecognizer(emotions=['angry', 'sad', 'neutral', 'ps', 'happy'], n_rnn_layers=2,\n", "    #                                 n_dense_layers=2, rnn_units=128, dense_units=128)\n", "    deeprec = DeepEmotionRecognizer(emotions=['sad', 'neutral', 'happy'], n_rnn_layers=2,\n", "                                    n_dense_layers=2, rnn_units=128, dense_units=128)\n", "    # train the model\n", "    deeprec.train()\n", "    # # get the accuracy\n", "    print(\"Test accuracy score: {:.3f}%\".format(deeprec.test_score() * 100))\n", "    print(deeprec.confusion_matrix(percentage=False, labeled=True))\n\n", "    # filename = \"data/my-custom/female1_1a_1_concerned.wav\"\n", "    # # filename2= \"test.wav\"\n", "    # # record_to_file(filename)\n", "    # result = detector.predict_proba(filename)\n", "    # print(\"Results:\")\n", "    # for emotion, prob in result.items():\n", "    #     print(f\"{emotion}: {prob*100}%\")\n", "    # # print(result)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}