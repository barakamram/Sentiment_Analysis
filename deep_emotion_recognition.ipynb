{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "# disable keras loggings\n", "import sys\n", "stderr = sys.stderr\n", "sys.stderr = open(os.devnull, 'w')\n", "import tensorflow as tf\n", "from tensorflow import keras"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tensorflow.keras.layers import LSTM, GRU, Dense, Activation, LeakyReLU, Dropout\n", "from tensorflow.keras.layers import Conv1D, MaxPool1D, GlobalAveragePooling1D\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n", "from tensorflow.keras.utils import to_categorical"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score, mean_absolute_error, confusion_matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from data_extractor import load_data\n", "from create_csv import write_custom_csv, write_emodb_csv, write_tess_ravdess_csv\n", "from emotion_recognition import EmotionRecognizer\n", "from utils import get_first_letters, AVAILABLE_EMOTIONS, extract_feature, get_dropout_str"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import random"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["physical_devices = tf.config.list_physical_devices(\"GPU\")\n", "print(f'Number of GPU detected: {len(physical_devices)}')\n", "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices(\"GPU\")[0], True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DeepEmotionRecognizer(EmotionRecognizer):\n", "    \"\"\"\n", "    The Deep Learning version of the Emotion Recognizer.\n", "    This class uses RNN (LSTM, GRU, etc.) and Dense layers.\n", "    #TODO add CNNs\n", "    \"\"\"\n", "    def __init__(self, **kwargs):\n", "        \"\"\"\n", "        params:\n", "            emotions (list): list of emotions to be used. Note that these emotions must be available in\n", "                RAVDESS_TESS & EMODB Datasets, available nine emotions are the following:\n", "                    'neutral', 'calm', 'happy', 'sad', 'angry', 'fear', 'disgust', 'ps' ( pleasant surprised ), 'boredom'.\n", "                Default is [\"sad\", \"neutral\", \"happy\"].\n", "            tess_ravdess (bool): whether to use TESS & RAVDESS Speech datasets, default is True.\n", "            emodb (bool): whether to use EMO-DB Speech dataset, default is True.\n", "            custom_db (bool): whether to use custom Speech dataset that is located in `data/train-custom`\n", "                and `data/test-custom`, default is True.\n", "            tess_ravdess_name (str): the name of the output CSV file for TESS&RAVDESS dataset, default is \"tess_ravdess.csv\".\n", "            emodb_name (str): the name of the output CSV file for EMO-DB dataset, default is \"emodb.csv\".\n", "            custom_db_name (str): the name of the output CSV file for the custom dataset, default is \"custom.csv\".\n", "            features (list): list of speech features to use, default is [\"mfcc\", \"chroma\", \"mel\"]\n", "                (i.e MFCC, Chroma and MEL spectrogram ).\n", "            classification (bool): whether to use classification or regression, default is True.\n", "            balance (bool): whether to balance the dataset ( both training and testing ), default is True.\n", "            verbose (bool/int): whether to print messages on certain tasks.\n", "            ==========================================================\n", "            Model params\n", "            n_rnn_layers (int): number of RNN layers, default is 2.\n", "            cell (keras.layers.RNN instance): RNN cell used to train the model, default is LSTM.\n", "            rnn_units (int): number of units of `cell`, default is 128.\n", "            n_dense_layers (int): number of Dense layers, default is 2.\n", "            dense_units (int): number of units of the Dense layers, default is 128.\n", "            dropout (list/float): dropout rate,\n", "                - if list, it indicates the dropout rate of each layer.\n", "                - if float, it indicates the dropout rate for all layers.\n", "                Default is 0.3.\n", "            ==========================================================\n", "            Training params\n", "            batch_size (int): number of samples per gradient update, default is 64.\n", "            epochs (int): number of epochs, default is 1000.\n", "            optimizer (str/keras.optimizers.Optimizer instance): optimizer used to train, default is \"adam\".\n", "            loss (str/callback from keras.losses): loss function that is used to minimize during training,\n", "                default is \"categorical_crossentropy\" for classification and \"mean_squared_error\" for \n", "                regression.\n", "        \"\"\"\n", "        # init EmotionRecognizer\n", "        super().__init__(**kwargs)\n", "        self.n_rnn_layers = kwargs.get(\"n_rnn_layers\", 2)\n", "        self.n_dense_layers = kwargs.get(\"n_dense_layers\", 2)\n", "        self.rnn_units = kwargs.get(\"rnn_units\", 128)\n", "        self.dense_units = kwargs.get(\"dense_units\", 128)\n", "        self.cell = kwargs.get(\"cell\", LSTM)\n\n", "        # list of dropouts of each layer\n", "        # must be len(dropouts) = n_rnn_layers + n_dense_layers\n", "        self.dropout = kwargs.get(\"dropout\", 0.3)\n", "        self.dropout = self.dropout if isinstance(self.dropout, list) else [self.dropout] * ( self.n_rnn_layers + self.n_dense_layers )\n", "        # number of classes ( emotions )\n", "        self.output_dim = len(self.emotions)\n\n", "        # optimization attributes\n", "        self.optimizer = kwargs.get(\"optimizer\", \"adam\")\n", "        self.loss = kwargs.get(\"loss\", \"categorical_crossentropy\")\n\n", "        # training attributes\n", "        self.batch_size = kwargs.get(\"batch_size\", 64)\n", "        self.epochs = kwargs.get(\"epochs\", 500)\n", "        \n", "        # the name of the model\n", "        self.model_name = \"\"\n", "        self._update_model_name()\n\n", "        # init the model\n", "        self.model = None\n\n", "        # compute the input length\n", "        self._compute_input_length()\n\n", "        # boolean attributes\n", "        self.model_created = False\n", "        \n", "        # self.data_loaded = False\n", "    def _update_model_name(self):\n", "        \"\"\"\n", "        Generates a unique model name based on parameters passed and put it on `self.model_name`.\n", "        This is used when saving the model.\n", "        \"\"\"\n", "        # get first letters of emotions, for instance:\n", "        # [\"sad\", \"neutral\", \"happy\"] => 'HNS' (sorted alphabetically)\n", "        emotions_str = get_first_letters(self.emotions)\n", "        # 'c' for classification & 'r' for regression\n", "        problem_type = 'c' if self.classification else 'r'\n", "        dropout_str = get_dropout_str(self.dropout, n_layers=self.n_dense_layers + self.n_rnn_layers)\n", "        self.model_name = f\"{emotions_str}-{problem_type}-{self.cell.__name__}-layers-{self.n_rnn_layers}-{self.n_dense_layers}-units-{self.rnn_units}-{self.dense_units}-dropout-{dropout_str}.h5\"\n", "    def _get_model_filename(self):\n", "        \"\"\"Returns the relative path of this model name\"\"\"\n", "        return f\"results/{self.model_name}\"\n", "    def _model_exists(self):\n", "        \"\"\"\n", "        Checks if model already exists in disk, returns the filename,\n", "        and returns `None` otherwise.\n", "        \"\"\"\n", "        filename = self._get_model_filename()\n", "        return filename if os.path.isfile(filename) else None\n", "    def _compute_input_length(self):\n", "        \"\"\"\n", "        Calculates the input shape to be able to construct the model.\n", "        \"\"\"\n", "        if not self.data_loaded:\n", "            self.load_data()\n", "        self.input_length = self.X_train[0].shape[1]\n", "        # self.input_length = 180\n", "        print(self.X_train[0].shape)\n", "    def _verify_emotions(self):\n", "        super()._verify_emotions()\n", "        self.int2emotions = {i: e for i, e in enumerate(self.emotions)}\n", "        self.emotions2int = {v: k for k, v in self.int2emotions.items()}\n", "    def create_model(self):\n", "        \"\"\"\n", "        Constructs the neural network based on parameters passed.\n", "        \"\"\"\n", "        if self.model_created:\n", "            # model already created, why call twice\n", "            return\n", "        model = Sequential()\n", "        if not self.data_loaded:\n", "            # if data isn't loaded yet, load it\n", "            self.load_data()\n", "        \n\n", "        # rnn layers\n", "        for i in range(self.n_rnn_layers):\n", "            if i == 0:\n", "                # first layer\n", "                model.add(self.cell(self.rnn_units, return_sequences=True, input_shape=(None, self.input_length)))\n", "                model.add(Dropout(self.dropout[i]))\n", "            else:\n", "                # middle layers\n", "                model.add(self.cell(self.rnn_units, return_sequences=True))\n", "                model.add(Dropout(self.dropout[i]))\n", "        if self.n_rnn_layers == 0:\n", "            i = 0\n\n", "        # dense layers\n", "        for j in range(self.n_dense_layers):\n", "            # if n_rnn_layers = 0, only dense\n", "            if self.n_rnn_layers == 0 and j == 0:\n", "                model.add(Dense(self.dense_units, input_shape=(None, self.input_length)))\n", "                # model.add(Dropout(self.dropout[i+j]))\n", "            else:\n", "                model.add(Dense(self.dense_units))\n", "                # model.add(Dropout(self.dropout[i+j]))\n", "                \n", "        if self.classification:\n", "            model.add(Dense(self.output_dim, activation=\"softmax\"))\n", "            model.compile(loss=self.loss, metrics=[\"accuracy\"], optimizer=self.optimizer)\n", "        else:\n", "            model.add(Dense(1, activation=\"linear\"))\n", "            model.compile(loss=\"mean_squared_error\", metrics=[\"mean_absolute_error\"], optimizer=self.optimizer)\n", "        self.model = model\n", "        self.model_created = True\n", "        if self.verbose > 0:\n", "            print(f'[+] Model created')\n", "    def load_data(self):\n", "        \"\"\"\n", "        Loads and extracts features from the audio files for the db's specified.\n", "        And then reshapes the data.\n", "        \"\"\"\n", "        super().load_data()\n", "        # reshape X's to 3 dims\n", "        X_train_shape = self.X_train.shape\n", "        X_test_shape = self.X_test.shape\n", "        # self.X_train = self.X_train\n", "        print(len(self.X_train))\n", "        # for x in self.X_train:\n", "        # self.X_train = self.X_train.reshape((X_train_shape[0], X_train_shape[1],1))\n", "        # self.X_test = self.X_test.reshape(( X_test_shape[0], X_test_shape[1],1))\n\n", "        # self.X_train = self.X_train.reshape((X_train_shape[0], 1, X_train_shape[1]))\n", "        # self.X_test = self.X_test.reshape((X_test_shape[0], 1, X_test_shape[1]))\n", "        self.X_train = self.X_train.reshape((1, X_train_shape[0], X_train_shape[1]))\n", "        self.X_test = self.X_test.reshape((1, X_test_shape[0], X_test_shape[1]))\n", "        print(len(self.X_train))\n", "        # print(self.X_train[0])\n", "        # self.X_train = self.X_train[0].reshape(1, X_train_shape[0], X_train_shape[1])\n", "        # self.X_train = self.X_train[0]\n", "        # print(len(self.X_train[0]))\n\n", "        # self.X_test = self.X_test[0]\n", "        # print(self.X_train)\n", "        if self.classification:\n", "            # one-hot encode when its classification\n", "            self.y_train = to_categorical([ self.emotions2int[str(e)] for e in self.y_train ])\n", "            self.y_test = to_categorical([ self.emotions2int[str(e)] for e in self.y_test ])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        # reshape labels\n", "        y_train_shape = self.y_train.shape\n", "        y_test_shape = self.y_test.shape\n", "        if self.classification:\n", "            # self.y_train = self.y_train.reshape((y_train_shape[0], y_train_shape[1], 1))\n", "            # self.y_test = self.y_test.reshape((y_test_shape[0], y_test_shape[1],1))\n\n", "            # self.y_train = self.y_train.reshape((y_train_shape[0],1, y_train_shape[1]))\n", "            # self.y_test = self.y_test.reshape((y_test_shape[0],1, y_test_shape[1]))\n", "            self.y_train = self.y_train.reshape((1, y_train_shape[0], y_train_shape[1]))\n", "            self.y_test = self.y_test.reshape((1, y_test_shape[0], y_test_shape[1]))\n", "        else:\n", "            # self.y_train = self.y_train.reshape((y_train_shape[0], 1, 1))\n", "            # self.y_test = self.y_test.reshape(( y_test_shape[0],1, 1))\n", "            self.y_train = self.y_train.reshape((1, y_train_shape[0], 1))\n", "            self.y_test = self.y_test.reshape((1, y_test_shape[0], 1))\n", "        print(len(self.y_train))\n", "        print(len(self.y_train[0]))\n", "    def train(self, override=False):\n", "        \"\"\"\n", "        Trains the neural network.\n", "        Params:\n", "            override (bool): whether to override the previous identical model, can be used\n", "                when you changed the dataset, default is False\n", "        \"\"\"\n\n", "        # if model isn't created yet, create it\n", "        if not self.model_created:\n", "            self.create_model()\n", "        # if the model already exists and trained, just load the weights and return\n", "        # but if override is True, then just skip loading weights\n", "        if not override:\n", "            model_name = self._model_exists()\n", "            if model_name:\n", "                self.model.load_weights(model_name)\n", "                self.model_trained = True\n", "                if self.verbose > 0:\n", "                    print(\"[*] Model weights loaded\")\n", "                return\n", "        if not os.path.isdir(\"results\"):\n", "            os.mkdir(\"results\")\n", "        if not os.path.isdir(\"logs\"):\n", "            os.mkdir(\"logs\")\n", "        model_filename = self._get_model_filename()\n", "        # print(model_filename)\n", "        self.checkpointer = ModelCheckpoint(model_filename, save_best_only=False, verbose=1)\n", "        self.tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", self.model_name))\n\n", "        # self.history = self.model.fit(y_train, y_train,\n", "        #                               batch_size=self.batch_size,\n", "        #                               epochs=self.epochs,\n", "        #                               validation_data=(x_test, y_test),\n", "        #                               # callbacks=[self.checkpointer, self.tensorboard],\n", "        #                               verbose=self.verbose)\n", "        # print(self.X_train[0])\n\n", "        # big = list()\n", "        # for e in self.X_train[0]:\n", "        #     e = [e]\n", "        #     big.append(e[0])\n", "        #\n", "        # print(len(big))\n", "        # print(len(big[0]))\n", "        # # print(big[0])\n", "        # bigy = list()\n", "        # for e in self.y_train[0]:\n", "        #     e = [e]\n", "        #     bigy.append(e[0])\n", "        # print(len(bigy))\n", "        # print(len(bigy[0]))\n", "        # print(len(self.y_train[0][0]))\n", "        # # print(len(self.X_train))\n", "        # # print(len(self.X_train[0][0]))\n", "        # # print(len(self.y_train))\n", "        # print(self.y_train[0][0])\n", "        # print(bigy[0])\n\n", "        # self.X_train = self.X_train[0]\n", "        # self.X_train = [self.X_train]\n\n", "        # self.y_train = list(self.y_train[0])\n", "        # self.X_test = list(self.X_test[0])\n", "        # self.y_test = list(self.y_test[0])\n", "        # print(self.X_train)\n", "        self.model.fit(x=self.X_train, y=self.y_train,\n", "                        batch_size=1024,\n", "                        epochs=20,\n", "                        validation_data=(self.X_test, self.y_test),\n", "                        callbacks=[self.checkpointer, self.tensorboard],\n", "                        verbose=1)\n", "        self.model_trained = True\n", "        if self.verbose > 0:\n", "            print(\"[+] Model trained\")\n", "    def predict(self, audio_path):\n", "        feature = extract_feature(audio_path, **self.audio_config).reshape((1, 1, self.input_length))\n", "        if self.classification:\n", "            prediction = self.model.predict(feature)\n", "            prediction = np.argmax(np.squeeze(prediction))\n", "            return self.int2emotions[prediction]\n", "        else:\n", "            return np.squeeze(self.model.predict(feature))\n", "    def predict_proba(self, audio_path):\n", "        if self.classification:\n", "            feature = extract_feature(audio_path, **self.audio_config).reshape((1, 1, self.input_length))\n", "            proba = self.model.predict(feature)[0][0]\n", "            result = {}\n", "            for prob, emotion in zip(proba, self.emotions):\n", "                result[emotion] = prob\n", "            return result\n", "        else:\n", "            raise NotImplementedError(\"Probability prediction doesn't make sense for regression\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    def test_score(self):\n", "        y_test = self.y_test[0]\n", "        if self.classification:\n", "            y_pred = self.model.predict(self.X_test)[0]\n", "            y_pred = [np.argmax(y, out=None, axis=None) for y in y_pred]\n", "            y_test = [np.argmax(y, out=None, axis=None) for y in y_test]\n", "            return accuracy_score(y_true=y_test, y_pred=y_pred)\n", "        else:\n", "            y_pred = self.model.predict(self.X_test)[0]\n", "            return mean_absolute_error(y_true=y_test, y_pred=y_pred)\n", "    def train_score(self):\n", "        y_train = self.y_train[0]\n", "        if self.classification:\n", "            y_pred = self.model.predict(self.X_train)[0]\n", "            y_pred = [np.argmax(y, out=None, axis=None) for y in y_pred]\n", "            y_train = [np.argmax(y, out=None, axis=None) for y in y_train]\n", "            return accuracy_score(y_true=y_train, y_pred=y_pred)\n", "        else:\n", "            y_pred = self.model.predict(self.X_train)[0]\n", "            return mean_absolute_error(y_true=y_train, y_pred=y_pred)\n", "    def confusion_matrix(self, percentage=True, labeled=True):\n", "        \"\"\"Compute confusion matrix to evaluate the test accuracy of the classification\"\"\"\n", "        if not self.classification:\n", "            raise NotImplementedError(\"Confusion matrix works only when it is a classification problem\")\n", "        y_pred = self.model.predict(self.X_test)[0]\n", "        y_pred = np.array([ np.argmax(y, axis=None, out=None) for y in y_pred])\n", "        # invert from keras.utils.to_categorical\n", "        y_test = np.array([ np.argmax(y, axis=None, out=None) for y in self.y_test[0] ])\n", "        matrix = confusion_matrix(y_test, y_pred, labels=[self.emotions2int[e] for e in self.emotions]).astype(np.float32)\n", "        if percentage:\n", "            for i in range(len(matrix)):\n", "                matrix[i] = matrix[i] / np.sum(matrix[i])\n", "            # make it percentage\n", "            matrix *= 100\n", "        if labeled:\n", "            matrix = pd.DataFrame(matrix, index=[ f\"true_{e}\" for e in self.emotions ],\n", "                                    columns=[ f\"predicted_{e}\" for e in self.emotions ])\n", "        return matrix\n", "    def get_n_samples(self, emotion, partition):\n", "        \"\"\"Returns number data samples of the `emotion` class in a particular `partition`\n", "        ('test' or 'train')\n", "        \"\"\"\n", "        if partition == \"test\":\n", "            if self.classification:\n", "                y_test = np.array([ np.argmax(y, axis=None, out=None)+1 for y in np.squeeze(self.y_test) ]) \n", "            else:\n", "                y_test = np.squeeze(self.y_test)\n", "            return len([y for y in y_test if y == emotion])\n", "        elif partition == \"train\":\n", "            if self.classification:\n", "                y_train = np.array([ np.argmax(y, axis=None, out=None)+1 for y in np.squeeze(self.y_train) ])\n", "            else:\n", "                y_train = np.squeeze(self.y_train)\n", "            return len([y for y in y_train if y == emotion])\n", "    def get_samples_by_class(self):\n", "        \"\"\"\n", "        Returns a dataframe that contains the number of training \n", "        and testing samples for all emotions\n", "        \"\"\"\n", "        train_samples = []\n", "        test_samples = []\n", "        total = []\n", "        for emotion in self.emotions:\n", "            n_train = self.get_n_samples(self.emotions2int[emotion]+1, \"train\")\n", "            n_test = self.get_n_samples(self.emotions2int[emotion]+1, \"test\")\n", "            train_samples.append(n_train)\n", "            test_samples.append(n_test)\n", "            total.append(n_train + n_test)\n", "        \n", "        # get total\n", "        total.append(sum(train_samples) + sum(test_samples))\n", "        train_samples.append(sum(train_samples))\n", "        test_samples.append(sum(test_samples))\n", "        return pd.DataFrame(data={\"train\": train_samples, \"test\": test_samples, \"total\": total}, index=self.emotions + [\"total\"])\n", "    def get_random_emotion(self, emotion, partition=\"train\"):\n", "        \"\"\"\n", "        Returns random `emotion` data sample index on `partition`\n", "        \"\"\"\n", "        if partition == \"train\":\n", "            y_train = self.y_train[0]\n", "            index = random.choice(list(range(len(y_train))))\n", "            element = self.int2emotions[np.argmax(y_train[index])]\n", "            while element != emotion:\n", "                index = random.choice(list(range(len(y_train))))\n", "                element = self.int2emotions[np.argmax(y_train[index])]\n", "        elif partition == \"test\":\n", "            y_test = self.y_test[0]\n", "            index = random.choice(list(range(len(y_test))))\n", "            element = self.int2emotions[np.argmax(y_test[index])]\n", "            while element != emotion:\n", "                index = random.choice(list(range(len(y_test))))\n", "                element = self.int2emotions[np.argmax(y_test[index])]\n", "        else:\n", "            raise TypeError(\"Unknown partition, only 'train' or 'test' is accepted\")\n", "        return index\n", "    def determine_best_model(self):\n", "        # TODO\n", "        # raise TypeError(\"This method isn't supported yet for deep nn\")\n", "        pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    rec = DeepEmotionRecognizer(emotions=['angry', 'sad', 'neutral', 'ps', 'happy'],\n", "                                epochs=300, verbose=0)\n", "    # rec.train(override=False)\n", "    print(rec)\n", "    print(\"Test accuracy score:\", rec.test_score() * 100, \"%\")\n", "    rec.train()\n", "    print(\"Test accuracy score:\", rec.test_score() * 100, \"%\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}