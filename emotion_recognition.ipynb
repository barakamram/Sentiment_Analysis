{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from data_extractor import load_data\n", "from utils import extract_feature, AVAILABLE_EMOTIONS\n", "from create_csv import write_emodb_csv, write_tess_ravdess_csv, write_custom_csv"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score, make_scorer, fbeta_score, mean_squared_error, mean_absolute_error\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.model_selection import GridSearchCV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as pl\n", "from time import time\n", "from utils import get_best_estimators, get_audio_config\n", "import numpy as np\n", "import tqdm\n", "import os\n", "import random\n", "import pandas as pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class EmotionRecognizer:\n", "    \"\"\"A class for training, testing and predicting emotions based on\n", "    speech's features that are extracted and fed into `sklearn` or `keras` model\"\"\"\n", "    def __init__(self, model=None, **kwargs):\n", "        \"\"\"\n", "        Params:\n", "            model (sklearn model): the model used to detect emotions. If `model` is None, then self.determine_best_model()\n", "                will be automatically called\n", "            emotions (list): list of emotions to be used. Note that these emotions must be available in\n", "                RAVDESS_TESS & EMODB Datasets, available nine emotions are the following:\n", "                    'neutral', 'calm', 'happy', 'sad', 'angry', 'fear', 'disgust', 'ps' ( pleasant surprised ), 'boredom'.\n", "                Default is [\"sad\", \"neutral\", \"happy\"].\n", "            tess_ravdess (bool): whether to use TESS & RAVDESS Speech datasets, default is True\n", "            emodb (bool): whether to use EMO-DB Speech dataset, default is True,\n", "            custom_db (bool): whether to use custom Speech dataset that is located in `data/train-custom`\n", "                and `data/test-custom`, default is True\n", "            tess_ravdess_name (str): the name of the output CSV file for TESS&RAVDESS dataset, default is \"tess_ravdess.csv\"\n", "            emodb_name (str): the name of the output CSV file for EMO-DB dataset, default is \"emodb.csv\"\n", "            custom_db_name (str): the name of the output CSV file for the custom dataset, default is \"custom.csv\"\n", "            features (list): list of speech features to use, default is [\"mfcc\", \"chroma\", \"mel\"]\n", "                (i.e MFCC, Chroma and MEL spectrogram )\n", "            classification (bool): whether to use classification or regression, default is True\n", "            balance (bool): whether to balance the dataset ( both training and testing ), default is True\n", "            verbose (bool/int): whether to print messages on certain tasks, default is 1\n", "        Note that when `tess_ravdess`, `emodb` and `custom_db` are set to `False`, `tess_ravdess` will be set to True\n", "        automatically.\n", "        \"\"\"\n", "        # emotions\n", "        # self.emotions = kwargs.get(\"emotions\", ['angry', 'sad', 'neutral', 'ps', 'happy'])\n", "        self.emotions = kwargs.get(\"emotions\", ['sad', 'neutral', 'happy'])\n", "        # make sure that there are only available emotions\n", "        self._verify_emotions()\n", "        # audio config\n", "        self.features = kwargs.get(\"features\", [\"mfcc\", \"chroma\", \"mel\"])\n", "        self.audio_config = get_audio_config(self.features)\n", "        # datasets\n", "        self.tess_ravdess = kwargs.get(\"tess_ravdess\", True)\n", "        self.emodb = kwargs.get(\"emodb\", True)\n", "        self.custom_db = kwargs.get(\"custom_db\", True)\n", "        if not self.tess_ravdess and not self.emodb and not self.custom_db:\n", "            self.tess_ravdess = True\n", "    \n", "        self.classification = kwargs.get(\"classification\", True)\n", "        self.balance = kwargs.get(\"balance\", True)\n", "        self.override_csv = kwargs.get(\"override_csv\", True)\n", "        self.verbose = kwargs.get(\"verbose\", 1)\n", "        self.tess_ravdess_name = kwargs.get(\"tess_ravdess_name\", \"tess_ravdess.csv\")\n", "        self.emodb_name = kwargs.get(\"emodb_name\", \"emodb.csv\")\n", "        self.custom_db_name = kwargs.get(\"custom_db_name\", \"custom.csv\")\n", "        self.verbose = kwargs.get(\"verbose\", 1)\n\n", "        # set metadata path file names\n", "        self._set_metadata_filenames()\n", "        # write csv's anyway\n", "        self.write_csv()\n\n", "        # boolean attributes\n", "        self.data_loaded = False\n", "        self.model_trained = False\n\n", "        # model\n", "        if not model:\n", "            self.determine_best_model()\n", "        else:\n", "            self.model = model\n", "    def _set_metadata_filenames(self):\n", "        \"\"\"\n", "        Protected method to get all CSV (metadata) filenames into two instance attributes:\n", "        - `self.train_desc_files` for training CSVs\n", "        - `self.test_desc_files` for testing CSVs\n", "        \"\"\"\n", "        train_desc_files, test_desc_files = [], []\n", "        if self.tess_ravdess:\n", "            train_desc_files.append(f\"train_{self.tess_ravdess_name}\")\n", "            test_desc_files.append(f\"test_{self.tess_ravdess_name}\")\n", "        if self.emodb:\n", "            train_desc_files.append(f\"train_{self.emodb_name}\")\n", "            test_desc_files.append(f\"test_{self.emodb_name}\")\n", "        if self.custom_db:\n", "            train_desc_files.append(f\"train_{self.custom_db_name}\")\n", "            test_desc_files.append(f\"test_{self.custom_db_name}\")\n\n", "        # set them to be object attributes\n", "        self.train_desc_files = train_desc_files\n", "        self.test_desc_files  = test_desc_files\n", "    def _verify_emotions(self):\n", "        \"\"\"\n", "        This method makes sure that emotions passed in parameters are valid.\n", "        \"\"\"\n", "        for emotion in self.emotions:\n", "            assert emotion in AVAILABLE_EMOTIONS, \"Emotion not recognized.\"\n", "    def get_best_estimators(self):\n", "        \"\"\"Loads estimators from grid files and returns them\"\"\"\n", "        return get_best_estimators(self.classification)\n", "    def write_csv(self):\n", "        \"\"\"\n", "        Write available CSV files in `self.train_desc_files` and `self.test_desc_files`\n", "        determined by `self._set_metadata_filenames()` method.\n", "        \"\"\"\n", "        for train_csv_file, test_csv_file in zip(self.train_desc_files, self.test_desc_files):\n", "            # not safe approach\n", "            if os.path.isfile(train_csv_file) and os.path.isfile(test_csv_file):\n", "                # file already exists, just skip writing csv files\n", "                if not self.override_csv:\n", "                    continue\n", "            if self.emodb_name in train_csv_file:\n", "                write_emodb_csv(self.emotions, train_name=train_csv_file, test_name=test_csv_file, verbose=self.verbose)\n", "                if self.verbose:\n", "                    print(\"[+] Generated EMO-DB CSV File\")\n", "            elif self.tess_ravdess_name in train_csv_file:\n", "                write_tess_ravdess_csv(self.emotions, train_name=train_csv_file, test_name=test_csv_file, verbose=self.verbose)\n", "                if self.verbose:\n", "                    print(\"[+] Generated TESS & RAVDESS DB CSV File\")\n", "            elif self.custom_db_name in train_csv_file:\n", "                write_custom_csv(emotions=self.emotions, train_name=train_csv_file, test_name=test_csv_file, verbose=self.verbose)\n", "                if self.verbose:\n", "                    print(\"[+] Generated Custom DB CSV File\")\n", "    def load_data(self):\n", "        \"\"\"\n", "        Loads and extracts features from the audio files for the db's specified\n", "        \"\"\"\n", "        if not self.data_loaded:\n", "            result = load_data(self.train_desc_files, self.test_desc_files, self.audio_config, self.classification,\n", "                                emotions=self.emotions, balance=self.balance)\n", "            self.X_train = result['X_train']\n", "            self.X_test = result['X_test']\n", "            self.y_train = result['y_train']\n", "            self.y_test = result['y_test']\n", "            self.train_audio_paths = result['train_audio_paths']\n", "            self.test_audio_paths = result['test_audio_paths']\n", "            self.balance = result[\"balance\"]\n", "            if self.verbose:\n", "                print(\"[+] Data loaded\")\n", "            self.data_loaded = True\n", "            # print(f'X_train ---------> {self.X_train}')\n", "            # print(f'y_train ---------> {self.y_train}')\n", "    def train(self, verbose=1):\n", "        \"\"\"\n", "        Train the model, if data isn't loaded, it 'll be loaded automatically\n", "        \"\"\"\n", "        if not self.data_loaded:\n", "            # if data isn't loaded yet, load it then\n", "            self.load_data()\n", "        print(len(self.X_train))\n", "        print(self.X_train[0])\n", "        if not self.model_trained:\n", "            self.model.fit(X=self.X_train, y=self.y_train)\n", "            self.model_trained = True\n", "            if verbose:\n", "                print(\"[+] Model trained\")\n", "    def predict(self, audio_path):\n", "        \"\"\"\n", "        given an `audio_path`, this method extracts the features\n", "        and predicts the emotion\n", "        \"\"\"\n", "        feature = extract_feature(audio_path, **self.audio_config).reshape(1, -1)\n", "        return self.model.predict(feature)[0]\n", "    def predict_proba(self, audio_path):\n", "        \"\"\"\n", "        Predicts the probability of each emotion.\n", "        \"\"\"\n", "        if self.classification:\n", "            feature = extract_feature(audio_path, **self.audio_config).reshape(1, -1)\n", "            proba = self.model.predict_proba(feature)[0]\n", "            result = {}\n", "            for emotion, prob in zip(self.model.classes_, proba):\n", "                result[emotion] = prob\n", "            return result\n", "        else:\n", "            raise NotImplementedError(\"Probability prediction doesn't make sense for regression\")\n", "    def grid_search(self, params, n_jobs=2, verbose=1):\n", "        \"\"\"\n", "        Performs GridSearchCV on `params` passed on the `self.model`\n", "        And returns the tuple: (best_estimator, best_params, best_score).\n", "        \"\"\"\n", "        score = accuracy_score if self.classification else mean_absolute_error\n", "        grid = GridSearchCV(estimator=self.model, param_grid=params, scoring=make_scorer(score),\n", "                            n_jobs=n_jobs, verbose=verbose, cv=3)\n", "        grid_result = grid.fit(self.X_train, self.y_train)\n", "        return grid_result.best_estimator_, grid_result.best_params_, grid_result.best_score_\n", "    def determine_best_model(self):\n", "        \"\"\"\n", "        Loads best estimators and determine which is best for test data,\n", "        and then set it to `self.model`.\n", "        In case of regression, the metric used is MSE and accuracy for classification.\n", "        Note that the execution of this method may take several minutes due\n", "        to training all estimators (stored in `grid` folder) for determining the best possible one.\n", "        \"\"\"\n", "        if not self.data_loaded:\n", "            self.load_data()\n", "        \n", "        # loads estimators\n", "        estimators = self.get_best_estimators()\n", "        result = []\n", "        if self.verbose:\n", "            estimators = tqdm.tqdm(estimators)\n", "        for estimator, params, cv_score in estimators:\n", "            if self.verbose:\n", "                estimators.set_description(f\"Evaluating {estimator.__class__.__name__}\")\n", "            detector = EmotionRecognizer(estimator, emotions=self.emotions, tess_ravdess=self.tess_ravdess,\n", "                                        emodb=self.emodb, custom_db=self.custom_db, classification=self.classification,\n", "                                        features=self.features, balance=self.balance, override_csv=False)\n", "            # data already loaded\n", "            detector.X_train = self.X_train\n", "            detector.X_test  = self.X_test\n", "            detector.y_train = self.y_train\n", "            detector.y_test  = self.y_test\n", "            detector.data_loaded = True\n", "            # train the model\n", "            detector.train(verbose=1)\n", "            # get test accuracy\n", "            accuracy = detector.test_score()\n", "            # append to result\n", "            result.append((detector.model, accuracy))\n\n", "        # sort the result\n", "        # regression: best is the lower, not the higher\n", "        # classification: best is higher, not the lower\n", "        result = sorted(result, key=lambda item: item[1], reverse=self.classification)\n", "        best_estimator = result[0][0]\n", "        accuracy = result[0][1]\n", "        self.model = best_estimator\n", "        self.model_trained = True\n", "        if self.verbose:\n", "            if self.classification:\n", "                print(f\"[+] Best model determined: {self.model.__class__.__name__} with {accuracy*100:.3f}% test accuracy\")\n", "            else:\n", "                print(f\"[+] Best model determined: {self.model.__class__.__name__} with {accuracy:.5f} mean absolute error\")\n", "    def test_score(self):\n", "        \"\"\"\n", "        Calculates score on testing data\n", "        if `self.classification` is True, the metric used is accuracy,\n", "        Mean-Squared-Error is used otherwise (regression)\n", "        \"\"\"\n", "        y_pred = self.model.predict(self.X_test)\n", "        if self.classification:\n", "            return accuracy_score(y_true=self.y_test, y_pred=y_pred)\n", "        else:\n", "            return mean_squared_error(y_true=self.y_test, y_pred=y_pred)\n", "    def train_score(self):\n", "        \"\"\"\n", "        Calculates accuracy score on training data\n", "        if `self.classification` is True, the metric used is accuracy,\n", "        Mean-Squared-Error is used otherwise (regression)\n", "        \"\"\"\n", "        y_pred = self.model.predict(self.X_train)\n", "        if self.classification:\n", "            return accuracy_score(y_true=self.y_train, y_pred=y_pred)\n", "        else:\n", "            return mean_squared_error(y_true=self.y_train, y_pred=y_pred)\n", "    def train_fbeta_score(self, beta):\n", "        y_pred = self.model.predict(self.X_train)\n", "        return fbeta_score(self.y_train, y_pred, beta, average='micro')\n", "    def test_fbeta_score(self, beta):\n", "        y_pred = self.model.predict(self.X_test)\n", "        return fbeta_score(self.y_test, y_pred, beta, average='micro')\n", "    def confusion_matrix(self, percentage=True, labeled=True):\n", "        \"\"\"\n", "        Computes confusion matrix to evaluate the test accuracy of the classification\n", "        and returns it as numpy matrix or pandas dataframe (depends on params).\n", "        params:\n", "            percentage (bool): whether to use percentage instead of number of samples, default is True.\n", "            labeled (bool): whether to label the columns and indexes in the dataframe.\n", "        \"\"\"\n", "        if not self.classification:\n", "            raise NotImplementedError(\"Confusion matrix works only when it is a classification problem\")\n", "        y_pred = self.model.predict(self.X_test)\n", "        matrix = confusion_matrix(self.y_test, y_pred, labels=self.emotions).astype(np.float32)\n", "        if percentage:\n", "            for i in range(len(matrix)):\n", "                matrix[i] = matrix[i] / np.sum(matrix[i])\n", "            # make it percentage\n", "            matrix *= 100\n", "        if labeled:\n", "            matrix = pd.DataFrame(matrix, index=[ f\"true_{e}\" for e in self.emotions ],\n", "                                    columns=[ f\"predicted_{e}\" for e in self.emotions ])\n", "        return matrix\n", "    def draw_confusion_matrix(self):\n", "        \"\"\"Calculates the confusion matrix and shows it\"\"\"\n", "        matrix = self.confusion_matrix(percentage=False, labeled=False)\n", "        #TODO: add labels, title, legends, etc.\n", "        pl.imshow(matrix, cmap=\"binary\")\n", "        pl.show()\n", "    def get_n_samples(self, emotion, partition):\n", "        \"\"\"Returns number data samples of the `emotion` class in a particular `partition`\n", "        ('test' or 'train')\n", "        \"\"\"\n", "        if partition == \"test\":\n", "            return len([y for y in self.y_test if y == emotion])\n", "        elif partition == \"train\":\n", "            return len([y for y in self.y_train if y == emotion])\n", "    def get_samples_by_class(self):\n", "        \"\"\"\n", "        Returns a dataframe that contains the number of training \n", "        and testing samples for all emotions.\n", "        Note that if data isn't loaded yet, it'll be loaded\n", "        \"\"\"\n", "        if not self.data_loaded:\n", "            self.load_data()\n", "        train_samples = []\n", "        test_samples = []\n", "        total = []\n", "        for emotion in self.emotions:\n", "            n_train = self.get_n_samples(emotion, \"train\")\n", "            n_test = self.get_n_samples(emotion, \"test\")\n", "            train_samples.append(n_train)\n", "            test_samples.append(n_test)\n", "            total.append(n_train + n_test)\n", "        \n", "        # get total\n", "        total.append(sum(train_samples) + sum(test_samples))\n", "        train_samples.append(sum(train_samples))\n", "        test_samples.append(sum(test_samples))\n", "        return pd.DataFrame(data={\"train\": train_samples, \"test\": test_samples, \"total\": total}, index=self.emotions + [\"total\"])\n", "    def get_random_emotion(self, emotion, partition=\"train\"):\n", "        \"\"\"\n", "        Returns random `emotion` data sample index on `partition`.\n", "        \"\"\"\n", "        if partition == \"train\":\n", "            index = random.choice(list(range(len(self.y_train))))\n", "            while self.y_train[index] != emotion:\n", "                index = random.choice(list(range(len(self.y_train))))\n", "        elif partition == \"test\":\n", "            index = random.choice(list(range(len(self.y_test))))\n", "            while self.y_train[index] != emotion:\n", "                index = random.choice(list(range(len(self.y_test))))\n", "        else:\n", "            raise TypeError(\"Unknown partition, only 'train' or 'test' is accepted\")\n", "        return index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_histograms(classifiers=True, beta=0.5, n_classes=3, verbose=1):\n", "    \"\"\"\n", "    Loads different estimators from `grid` folder and calculate some statistics to plot histograms.\n", "    Params:\n", "        classifiers (bool): if `True`, this will plot classifiers, regressors otherwise.\n", "        beta (float): beta value for calculating fbeta score for various estimators.\n", "        n_classes (int): number of classes\n", "    \"\"\"\n", "    # get the estimators from the performed grid search result\n", "    estimators = get_best_estimators(classifiers)\n", "    final_result = {}\n", "    for estimator, params, cv_score in estimators:\n", "        final_result[estimator.__class__.__name__] = []\n", "        for i in range(3):\n", "            result = {}\n", "            # initialize the class\n", "            detector = EmotionRecognizer(estimator, verbose=0)\n", "            # load the data\n", "            detector.load_data()\n", "            if i == 0:\n", "                # first get 1% of sample data\n", "                sample_size = 0.01\n", "            elif i == 1:\n", "                # second get 10% of sample data\n", "                sample_size = 0.1\n", "            elif i == 2:\n", "                # last get all the data\n", "                sample_size = 1\n", "            # calculate number of training and testing samples\n", "            n_train_samples = int(len(detector.X_train) * sample_size)\n", "            n_test_samples = int(len(detector.X_test) * sample_size)\n", "            # set the data\n", "            detector.X_train = detector.X_train[:n_train_samples]\n", "            detector.X_test = detector.X_test[:n_test_samples]\n", "            detector.y_train = detector.y_train[:n_train_samples]\n", "            detector.y_test = detector.y_test[:n_test_samples]\n", "            # calculate train time\n", "            t_train = time()\n", "            detector.train()\n", "            t_train = time() - t_train\n", "            # calculate test time\n", "            t_test = time()\n", "            test_accuracy = detector.test_score()\n", "            t_test = time() - t_test\n", "            # set the result to the dictionary\n", "            result['train_time'] = t_train\n", "            result['pred_time'] = t_test\n", "            result['acc_train'] = cv_score\n", "            result['acc_test'] = test_accuracy\n", "            result['f_train'] = detector.train_fbeta_score(beta)\n", "            result['f_test'] = detector.test_fbeta_score(beta)\n", "            if verbose:\n", "                print(f\"[+] {estimator.__class__.__name__} with {sample_size*100}% ({n_train_samples}) data samples achieved {cv_score*100:.3f}% Validation Score in {t_train:.3f}s & {test_accuracy*100:.3f}% Test Score in {t_test:.3f}s\")\n", "            # append the dictionary to the list of results\n", "            final_result[estimator.__class__.__name__].append(result)\n", "        if verbose:\n", "            print()\n", "    visualize(final_result, n_classes=n_classes)\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualize(results, n_classes):\n", "    \"\"\"\n", "    Visualization code to display results of various learners.\n", "    \n", "    inputs:\n", "      - results: a dictionary of lists of dictionaries that contain various results on the corresponding estimator\n", "      - n_classes: number of classes\n", "    \"\"\"\n", "    n_estimators = len(results)\n\n", "    # naive predictor\n", "    accuracy = 1 / n_classes\n", "    f1 = 1 / n_classes\n", "    # Create figure\n", "    fig, ax = pl.subplots(2, 4, figsize = (11,7))\n", "    # Constants\n", "    bar_width = 0.4\n", "    colors = [ (random.random(), random.random(), random.random()) for _ in range(n_estimators) ]\n", "    # Super loop to plot four panels of data\n", "    for k, learner in enumerate(results.keys()):\n", "        for j, metric in enumerate(['train_time', 'acc_train', 'f_train', 'pred_time', 'acc_test', 'f_test']):\n", "            for i in np.arange(3):\n", "                x = bar_width * n_estimators\n", "                # Creative plot code\n", "                ax[j//3, j%3].bar(i*x+k*(bar_width), results[learner][i][metric], width = bar_width, color = colors[k])\n", "                ax[j//3, j%3].set_xticks([x-0.2, x*2-0.2, x*3-0.2])\n", "                ax[j//3, j%3].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n", "                ax[j//3, j%3].set_xlabel(\"Training Set Size\")\n", "                ax[j//3, j%3].set_xlim((-0.2, x*3))\n", "    # Add unique y-labels\n", "    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n", "    ax[0, 1].set_ylabel(\"Accuracy Score\")\n", "    ax[0, 2].set_ylabel(\"F-score\")\n", "    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n", "    ax[1, 1].set_ylabel(\"Accuracy Score\")\n", "    ax[1, 2].set_ylabel(\"F-score\")\n", "    # Add titles\n", "    ax[0, 0].set_title(\"Model Training\")\n", "    ax[0, 1].set_title(\"Accuracy Score on Training Subset\")\n", "    ax[0, 2].set_title(\"F-score on Training Subset\")\n", "    ax[1, 0].set_title(\"Model Predicting\")\n", "    ax[1, 1].set_title(\"Accuracy Score on Testing Set\")\n", "    ax[1, 2].set_title(\"F-score on Testing Set\")\n", "    # Add horizontal lines for naive predictors\n", "    ax[0, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n", "    ax[1, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n", "    ax[0, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n", "    ax[1, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n", "    # Set y-limits for score panels\n", "    ax[0, 1].set_ylim((0, 1))\n", "    ax[0, 2].set_ylim((0, 1))\n", "    ax[1, 1].set_ylim((0, 1))\n", "    ax[1, 2].set_ylim((0, 1))\n", "    # Set additional plots invisibles\n", "    ax[0, 3].set_visible(False)\n", "    ax[1, 3].axis('off')\n", "    # Create legend\n", "    for i, learner in enumerate(results.keys()):\n", "        pl.bar(0, 0, color=colors[i], label=learner)\n", "    pl.legend()\n", "    # Aesthetics\n", "    pl.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 1.10)\n", "    pl.tight_layout()\n", "    pl.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}